---
youku_id: 
youtube_id: 
chapter: 4
title: DQN 神经网络
---

* 学习资料:
  * [全部代码](https://github.com/MorvanZhou/tutorials/tree/master/Reinforcement_learning_TUT/5_Deep_Q_Network)
  * [什么是 DQN 短视频](#)
  * 本节内容的模拟视频效果[Youtube](https://www.youtube.com/watch?v=cIb5BNaO85Y&index=4&list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O_), [优酷](http://v.youku.com/v_show/id_XMTg3NTI2ODU2OA==.html)

接着上节内容, 这节我们使用 Tensorflow
(如果还不了解 Tensorflow, 这里去往
[经典的 Tensorflow 视频教程]({% link _contents/pages/table-contents/machine-learning/tensorflow/tensorflow.html %}))
来搭建 DQN 当中的神经网络部分 (用来预测 Q 值).



本节内容包括:

* [两个神经网络](#two-nets)
* [神经网络结构](#nn-structure)
* [初始值](#init)
* [创建两个网络](#net)
* [添加层](#layer)


<h4 class="tut-h4-pad" id="two-nets">两个神经网络</h4>

为了使用 Tensorflow 来实现 DQN, 比较推荐的方式是搭建两个神经网络, `target_net` 用于预测 `q_target` 值, 他不会及时更新参数.
`eval_net` 用于预测 `q_eval`, 这个神经网络拥有最新的神经网络参数. 不过这两个神经网络结构是完全一样的, 只是里面的参数不一样.
在[这个短视频里](#), 能找到我们为什么要建立两个不同参数的神经网络.

<img class="course-image" src="/static/results/rl/4-2-1.png">

<h4 class="tut-h4-pad" id="nn-structure">神经网络结构</h4>

因为 DQN 的结构相比之前所讲的内容都不一样, 所以我们不使用继承来实现这次的功能.
这次我们创建一个 `DeepQNetwork` 的 class, 以及他神经网络部分的功能.

```python
class DeepQNetwork:
    # 初始化
    def __init__(self):

    # 使用后面的 _build_layers 和 _add_layer 建立神经网络
    def _build_net(self):

    # 使用后面的 _add_layer 建立神经网络层
    def _build_layers(self, inputs, action_size, trainable):

    # 创建每一层
    def _add_layer(self, inputs, in_size, out_size, activation_function=None, trainable=True):
```

<h4 class="tut-h4-pad" id="init">初始值</h4>

在创建神经网络时, 我们需要传入神经网络的结构, `hidden_layers` 就是为了实现这个目标的.
`hidden_layers=[10, 10]` 代表着我们除了输入层, 还有两个隐藏层, 神经元个数都是10.
而 `output_layer` 的结构比较特殊, 他的输出时所有 action 的值, 所以是一种固定形态, 我们之后会讲到.

```python
class DeepQNetwork:
    def __init__(self, hidden_layers=[10, 10]):
        self.hidden_layers = hidden_layers
```

<img class="course-image" src="/static/results/rl/4-2-2.png">


<h4 class="tut-h4-pad" id="net">创建两个网络</h4>

两个神经网络是为了固定住一个神经网络 (`target_net`) 的参数, `target_net` 是 `eval_net` 的一个历史版本,
拥有 `eval_net` 很久之前的一组参数, 而且这组参数被固定一段时间, 然后再被 `eval_net` 的新参数所替换.
而 `eval_net` 是不断在被提升的, 所以是一个可以被训练的网络 `trainable=True`. 而 `target_net` 的 `trainable=False`.

```python
class DeepQNetwork:
    def __init__(self, hidden_layers=[10, 10]):
        ...
    def _build_net(self):
        # 分别保存两个神经网络的参数
        self._eval_net_params = []
        self._target_net_params = []

        # 创建 eval 神经网络, 及时提升参数
        self.s = tf.placeholder(tf.float32, [None, self.n_features], name='s')  # 用来接收 observation
        self.q_target = tf.placeholder(tf.float32, [None, self.n_actions], name='Q_target') # 用来接收 q_target 的值, 这个之后会通过计算得到

        with tf.variable_scope('eval_net'):
            self.q_eval = self._build_layers(self.s, self.n_actions, trainable=True)    # 建模
            with tf.name_scope('loss'):
                self.loss = tf.reduce_sum(tf.square(self.q_target - self.q_eval))   # 求误差
            with tf.name_scope('train'):
                self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss) # 梯度下降

        # 创建 target 神经网络, 提供 target Q
        self.s_ = tf.placeholder(tf.float32, [None, self.n_features], name='s_')    # 接收下个 observation
        with tf.variable_scope('target_net'):
            self.q_next = self._build_layers(self.s_, self.n_actions, trainable=False)  # 建模, 但不 train 这个网络
```


<h4 class="tut-h4-pad" id="layer">添加层</h4>

添加层功能会根据之前定义的 `self.hidden_layers` 神经网络结构来添加隐藏层 (hidden layer),
而 output_layer 的定义是根据环境中的 RL 学着有多少个 action 来定义的.

```python
class DeepQNetwork:
    def __init__(self, hidden_layers=[10, 10]):
        ...
    def _build_net(self):
        ...
    def _build_layers(self, inputs, action_size, trainable):  # 根据 observation 输出所有 actions value
        layers_output = [inputs]
        for i, n_unit in enumerate(self.hidden_layers): # 根据 self.hidden_layers 给定的神经网络结构建模
            with tf.variable_scope('layer%i' % i):
                output = self._add_layer(
                    layers_output[i],
                    in_size=layers_output[i].get_shape()[1].value,
                    out_size=n_unit,
                    activation_function=tf.nn.relu,
                    trainable=trainable,
                )
                layers_output.append(output)
        with tf.variable_scope('output_layer'): # 对于 output 有特殊要求, 输出个数是 action 的个数, 表示所有 actions 的 value
            output = self._add_layer(
                layers_output[-1],
                in_size=layers_output[-1].get_shape()[1].value,
                out_size=action_size,
                activation_function=None,
                trainable=trainable
            )
        return output   # 输出所有 actions 的值

    # 每层 layer 都包含了 Wx+b 和 激励函数的过程
    def _add_layer(self, inputs, in_size, out_size, activation_function=None, trainable=True):
        # 创建 weights and biases
        Weights = tf.get_variable(
            name='weights',
            shape=[in_size, out_size],
            trainable=trainable,
            initializer=tf.truncated_normal_initializer(mean=0., stddev=0.3)
        )
        biases = tf.get_variable(
            name='biases',
            shape=[out_size],
            initializer=tf.constant_initializer(0.1),
            trainable=trainable
        )

        # 分开储存所有的 W b, 为了之后要定时把 eval_net 的参数值赋给 target_net
        if trainable is True:
            self._eval_net_params.append([Weights, biases])
        else:
            self._target_net_params.append([Weights, biases])

        Wx_plus_b = tf.matmul(inputs, Weights) + biases

        # 激励函数
        if activation_function is None:
            outputs = Wx_plus_b
        else:
            outputs = activation_function(Wx_plus_b)
        return outputs
```

如果想一次性看到全部代码, 请去我的 [Github](https://github.com/MorvanZhou/tutorials/tree/master/Reinforcement_learning_TUT/5_Deep_Q_Network)