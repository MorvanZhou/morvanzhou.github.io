---
youku_id: XMjY0NDkzMDg0MA
youtube_id: cw0USSxeEzw
bilibili_id: 15989997
title: Policy Gradients
description: "今天我们会来说说强化学习家族中另一类型算法, 叫做 Policy Gradients.强化学习是一个通过奖惩来学习正确行为的机制. 家族中有很多种不一样的成员,  有学习奖惩值, 根据自己认为的高价值选行为, 比如 Q learning, Deep Q Network, 也有不通过分析奖励值,  直接输出行为的方法, 这就是今天要说的 Policy Gradients 了.  甚至我们可以为 Policy  Gradients 加上一个神经网络来输出预测的动作. 对比起以值为基础的方法, Policy Gradients 直接输出动作的最大好处就是, 它能在一个连续区间内挑选动作, 而基于值的, 比如 Q-learning, 它如果在无穷多的动作中计算价值, 从而选择行为, 这, 它可吃不消."
publish-date: 2017-03-17
chapter: 4
thumbnail: /static/thumbnail/ML-intro/PG.png
post-headings:
  - 和以往的强化学习方法不同
  - 更新不同之处
  - 具体更新步骤
---

学习资料:
  * [强化学习教程]({% link _table-contents/machine-learning/reinforcement-learning.html %})
  * [强化学习模拟程序](https://www.youtube.com/watch?v=G5BDgzxfLvA&list=PLXO45tsB95cLYyEsEylpPvTY-8ErPt2O_){:target="_blank"}
  * [Policy Gradient Python 教程]({% link _tutorials/machine-learning/reinforcement-learning/5-1-policy-gradient-softmax1.md %})
  * [强化学习实战]({% link _tutorials/machine-learning/ML-practice/RL-build-arm-from-scratch1.md %})
  * 论文 [Policy gradient methods for reinforcement learning with function approximation.](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf){:target="_blank"}



欢迎观看有趣的机器学习系列视频, 今天我们会来说说强化学习家族中另一类型算法, 叫做 Policy Gradients.

**注: 本文不会涉及数学推导. 大家可以在很多其他地方找到优秀的数学推导文章.**

 {% include assign-heading.html %}


{% include tut-image.html image-name="PG01.png" %}


强化学习是一个通过奖惩来学习正确行为的机制. 家族中有很多种不一样的成员,  有学习奖惩值, 根据自己认为的高价值选行为,
比如 [Q learning]({% link _tutorials/machine-learning/ML-intro/4-03-q-learning.md %}),
[Deep Q Network]({% link _tutorials/machine-learning/ML-intro/4-06-DQN.md %}), 也有不通过分析奖励值,  直接输出行为的方法, 这就是今天要说的 Policy Gradients 了.
甚至我们可以为 Policy  Gradients 加上一个神经网络来输出预测的动作. 对比起以值为基础的方法, Policy Gradients 直接输出动作的最大好处就是, 它能在一个连续区间内挑选动作, 而基于值的, 比如 Q-learning, 它如果在无穷多的动作中计算价值, 从而选择行为, 这, 它可吃不消.


{% include assign-heading.html %}

{% include tut-image.html image-name="PG02.png" %}

有了神经网络当然方便, 但是,  我们怎么进行神经网络的误差反向传递呢? Policy Gradients 的误差又是什么呢? 答案是! 哈哈,  没有误差! 但是他的确是在进行某一种的反向传递.  这种反向传递的目的是让这次被选中的行为更有可能在下次发生. 但是我们要怎么确定这个行为是不是应当被增加被选的概率呢? 这时候我们的老朋友,  reward 奖惩正可以在这时候派上用场,


{% include google-in-article-ads.html %}

{% include assign-heading.html %}

{% include tut-image.html image-name="PG03.png" %}

现在我们来演示一遍, 观测的信息通过神经网络分析, 选出了左边的行为, 我们直接进行反向传递,  使之下次被选的可能性增加, 但是奖惩信息却告诉我们,  这次的行为是不好的, 那我们的动作可能性增加的幅度  随之被减低. 这样就能靠奖励来左右我们的神经网络反向传递. 我们再来举个例子, 假如这次的观测信息让神经网络选择了右边的行为,  右边的行为随之想要进行反向传递, 使右边的行为下次被多选一点, 这时,  奖惩信息也来了, 告诉我们这是好行为, 那我们就在这次反向传递的时候加大力度,  让它下次被多选的幅度更猛烈! 这就是 Policy Gradients 的核心思想了. 很简单吧.

